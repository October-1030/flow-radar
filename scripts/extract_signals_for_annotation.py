#!/usr/bin/env python3
"""
ä¿¡å·æå–å·¥å…· - ä¸ºäººå·¥æ ‡æ³¨å‡†å¤‡æ ·æœ¬

åŠŸèƒ½ï¼š
1. è¯»å– storage/events/*.jsonl.gz ä¸­çš„å†°å±±ä¿¡å·
2. æŠ½æ · N=30 ä¸ª CONFIRMED ä¿¡å·
3. è¾“å‡ºåˆ° CSV æ ¼å¼ï¼ˆä¾¿äº Google Sheet æ ‡æ³¨ï¼‰

æŠ½æ ·ç­–ç•¥ï¼š
- æ—¶é—´åˆ†å±‚ï¼š72h å¹³å‡åˆ† 6 ä¸ªæ—¶æ®µï¼Œæ¯æ—¶æ®µæŠ½ 5 ä¸ª
- ä¹°å–å¹³è¡¡ï¼šBUY/SELL å„çº¦ 15 ä¸ª
- ç½®ä¿¡åº¦è¦†ç›–ï¼šé«˜ï¼ˆ80%+ï¼‰ã€ä¸­ï¼ˆ60-80%ï¼‰ã€ä½ï¼ˆ<60%ï¼‰

è¾“å‡ºå­—æ®µï¼š
ts, symbol, side, level, confidence, price, cumulative_filled, refill_count,
intensity, key, snippet_path, offset

éš”ç¦»åŸåˆ™ï¼š
- åªåš"è¯»æ–‡ä»¶â†’æŠ½æ ·â†’å¯¼å‡º"
- ä¸ import è¿è¡Œä¸­çš„æ ¸å¿ƒæ¨¡å—ï¼ˆé¿å… side-effectï¼‰
- ä¸å½±å“å½“å‰ 72h éªŒè¯è¿è¡Œ

ä½œè€…ï¼šClaude Code
æ—¥æœŸï¼š2026-01-07
ç‰ˆæœ¬ï¼šv2.0 (ä¸‰æ–¹ä¼šè°ˆç¬¬äºŒåäºŒè½®å…±è¯†)
"""

import gzip
import json
import csv
import random
from pathlib import Path
from datetime import datetime
from typing import List, Dict
from collections import defaultdict


# ==================== é…ç½®å‚æ•° ====================

SAMPLE_SIZE = 30  # æŠ½æ ·æ€»æ•°
TIME_BUCKETS = 6  # æ—¶é—´åˆ†å±‚æ•°
SAMPLES_PER_BUCKET = 5  # æ¯æ—¶æ®µæŠ½æ ·æ•°

EVENTS_DIR = Path("storage/events")
OUTPUT_CSV = Path("docs/iceberg_annotation_samples.csv")

# CSV è¾“å‡ºå­—æ®µ
CSV_FIELDS = [
    'ts', 'symbol', 'side', 'level', 'confidence', 'price',
    'cumulative_filled', 'refill_count', 'intensity',
    'key', 'snippet_path', 'offset'
]


# ==================== äº‹ä»¶æ–‡ä»¶è¯»å– ====================

def read_iceberg_signals(events_dir: Path) -> List[Dict]:
    """
    è¯»å–æ‰€æœ‰å†°å±±ä¿¡å·äº‹ä»¶

    Args:
        events_dir: äº‹ä»¶æ–‡ä»¶ç›®å½•

    Returns:
        [(signal, snippet_path, offset), ...]
    """
    signals = []

    # éå†æ‰€æœ‰ .jsonl.gz æ–‡ä»¶
    event_files = sorted(events_dir.glob("*.jsonl.gz"))

    print(f"æ‰¾åˆ° {len(event_files)} ä¸ªäº‹ä»¶æ–‡ä»¶")

    for event_file in event_files:
        print(f"æ­£åœ¨è¯»å–: {event_file.name}")

        try:
            with gzip.open(event_file, 'rt', encoding='utf-8') as f:
                for line_num, line in enumerate(f, start=1):
                    try:
                        event = json.loads(line.strip())

                        # åªæå–å†°å±±ä¿¡å·
                        if event.get('type') == 'iceberg':
                            # æ·»åŠ å®šä½ä¿¡æ¯
                            signal_with_meta = {
                                **event,
                                'snippet_path': str(event_file),
                                'offset': line_num
                            }
                            signals.append(signal_with_meta)

                    except json.JSONDecodeError as e:
                        print(f"  è­¦å‘Š: {event_file.name}:{line_num} JSON è§£æå¤±è´¥: {e}")
                        continue

        except Exception as e:
            print(f"  é”™è¯¯: æ— æ³•è¯»å– {event_file.name}: {e}")
            continue

    print(f"\næ€»å…±è¯»å–åˆ° {len(signals)} ä¸ªå†°å±±ä¿¡å·")
    return signals


# ==================== ä¿¡å·è¿‡æ»¤ ====================

def filter_confirmed_signals(signals: List[Dict]) -> List[Dict]:
    """
    è¿‡æ»¤å‡º CONFIRMED çº§åˆ«çš„ä¿¡å·

    Args:
        signals: æ‰€æœ‰å†°å±±ä¿¡å·

    Returns:
        CONFIRMED ä¿¡å·åˆ—è¡¨
    """
    confirmed = []

    for sig in signals:
        # æ£€æŸ¥ level å­—æ®µï¼ˆå¯èƒ½åœ¨ data ä¸­æˆ–é¡¶å±‚ï¼‰
        level = sig.get('level') or sig.get('data', {}).get('level')

        if level == 'CONFIRMED':
            confirmed.append(sig)

    print(f"è¿‡æ»¤åå¾—åˆ° {len(confirmed)} ä¸ª CONFIRMED ä¿¡å·")
    return confirmed


# ==================== åˆ†å±‚æŠ½æ · ====================

def stratified_sampling(signals: List[Dict]) -> List[Dict]:
    """
    åˆ†å±‚æŠ½æ ·ç®—æ³•

    ç­–ç•¥ï¼š
    1. æŒ‰æ—¶é—´åˆ†å±‚ï¼ˆ6 ä¸ªæ—¶æ®µï¼‰
    2. æ¯æ—¶æ®µæŒ‰ä¹°å–æ–¹å‘åˆ†å±‚
    3. æ¯å±‚æŒ‰ç½®ä¿¡åº¦è¦†ç›–

    Args:
        signals: CONFIRMED ä¿¡å·åˆ—è¡¨

    Returns:
        æŠ½æ ·åçš„ä¿¡å·åˆ—è¡¨ï¼ˆæœ€å¤š 30 ä¸ªï¼‰
    """
    if not signals:
        print("è­¦å‘Š: æ²¡æœ‰å¯ç”¨ä¿¡å·è¿›è¡ŒæŠ½æ ·")
        return []

    # 1. è®¡ç®—æ—¶é—´èŒƒå›´
    timestamps = [sig['ts'] for sig in signals]
    min_ts = min(timestamps)
    max_ts = max(timestamps)
    time_range = max_ts - min_ts
    bucket_duration = time_range / TIME_BUCKETS if time_range > 0 else 1

    print(f"\næ—¶é—´èŒƒå›´: {datetime.fromtimestamp(min_ts)} åˆ° {datetime.fromtimestamp(max_ts)}")
    print(f"æ¯æ—¶æ®µæ—¶é•¿: {bucket_duration / 3600:.1f} å°æ—¶")

    # 2. æŒ‰æ—¶é—´å’Œæ–¹å‘åˆ†ç»„
    buckets = defaultdict(lambda: {'BUY': [], 'SELL': []})

    for sig in signals:
        # è®¡ç®—æ—¶é—´æ¡¶ç´¢å¼•
        bucket_idx = min(
            int((sig['ts'] - min_ts) / bucket_duration),
            TIME_BUCKETS - 1
        )

        # è·å–ä¹°å–æ–¹å‘
        side = sig.get('side') or sig.get('data', {}).get('side', 'BUY')

        buckets[bucket_idx][side].append(sig)

    # 3. ä»æ¯ä¸ªæ—¶æ®µæŠ½æ ·
    sampled = []

    for bucket_idx in range(TIME_BUCKETS):
        buy_signals = buckets[bucket_idx]['BUY']
        sell_signals = buckets[bucket_idx]['SELL']

        # è®¡ç®—æœ¬æ—¶æ®µåº”æŠ½æ ·æ•°
        target_buy = SAMPLES_PER_BUCKET // 2
        target_sell = SAMPLES_PER_BUCKET - target_buy

        # ä» BUY æŠ½æ ·
        if buy_signals:
            # æŒ‰ç½®ä¿¡åº¦æ’åºåæŠ½æ ·ï¼ˆè¦†ç›–é«˜ä¸­ä½ï¼‰
            buy_signals_sorted = sorted(
                buy_signals,
                key=lambda s: s.get('confidence', s.get('data', {}).get('confidence', 0))
            )
            sampled_buy = sample_with_coverage(buy_signals_sorted, target_buy)
            sampled.extend(sampled_buy)
            print(f"  æ—¶æ®µ {bucket_idx+1}: BUY æŠ½æ · {len(sampled_buy)}/{len(buy_signals)}")

        # ä» SELL æŠ½æ ·
        if sell_signals:
            sell_signals_sorted = sorted(
                sell_signals,
                key=lambda s: s.get('confidence', s.get('data', {}).get('confidence', 0))
            )
            sampled_sell = sample_with_coverage(sell_signals_sorted, target_sell)
            sampled.extend(sampled_sell)
            print(f"  æ—¶æ®µ {bucket_idx+1}: SELL æŠ½æ · {len(sampled_sell)}/{len(sell_signals)}")

    # 4. å¦‚æœä¸è¶³ 30 ä¸ªï¼Œè¡¥å……éšæœºæŠ½æ ·
    if len(sampled) < SAMPLE_SIZE:
        remaining = [s for s in signals if s not in sampled]
        need = min(SAMPLE_SIZE - len(sampled), len(remaining))
        if need > 0:
            additional = random.sample(remaining, need)
            sampled.extend(additional)
            print(f"\nè¡¥å……éšæœºæŠ½æ · {need} ä¸ªä¿¡å·")

    # 5. å¦‚æœè¶…è¿‡ 30 ä¸ªï¼Œéšæœºåˆ å‡
    if len(sampled) > SAMPLE_SIZE:
        sampled = random.sample(sampled, SAMPLE_SIZE)

    print(f"\næœ€ç»ˆæŠ½æ ·ç»“æœ: {len(sampled)} ä¸ªä¿¡å·")
    return sampled


def sample_with_coverage(signals: List[Dict], target: int) -> List[Dict]:
    """
    ä»æ’åºåçš„ä¿¡å·ä¸­æŠ½æ ·ï¼Œç¡®ä¿è¦†ç›–é«˜ä¸­ä½ç½®ä¿¡åº¦

    Args:
        signals: æŒ‰ç½®ä¿¡åº¦æ’åºçš„ä¿¡å·åˆ—è¡¨
        target: ç›®æ ‡æŠ½æ ·æ•°

    Returns:
        æŠ½æ ·ç»“æœ
    """
    if not signals:
        return []

    n = len(signals)
    if n <= target:
        return signals

    # åˆ†ä¸‰æ®µæŠ½æ ·ï¼ˆä½ã€ä¸­ã€é«˜ç½®ä¿¡åº¦ï¼‰
    low_end = n // 3
    mid_start = n // 3
    mid_end = 2 * n // 3
    high_start = 2 * n // 3

    samples_per_tier = max(1, target // 3)

    sampled = []

    # ä½ç½®ä¿¡åº¦æŠ½æ ·
    if low_end > 0:
        sampled.extend(random.sample(signals[:low_end], min(samples_per_tier, low_end)))

    # ä¸­ç½®ä¿¡åº¦æŠ½æ ·
    mid_signals = signals[mid_start:mid_end]
    if mid_signals:
        sampled.extend(random.sample(mid_signals, min(samples_per_tier, len(mid_signals))))

    # é«˜ç½®ä¿¡åº¦æŠ½æ ·
    high_signals = signals[high_start:]
    if high_signals:
        remaining = target - len(sampled)
        sampled.extend(random.sample(high_signals, min(remaining, len(high_signals))))

    return sampled


# ==================== CSV è¾“å‡º ====================

def export_to_csv(signals: List[Dict], output_path: Path):
    """
    å¯¼å‡ºä¿¡å·åˆ° CSV æ–‡ä»¶

    Args:
        signals: ä¿¡å·åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # æå–å­—æ®µå€¼
    rows = []
    for sig in signals:
        data = sig.get('data', {})

        row = {
            'ts': sig.get('ts', 0),
            'symbol': sig.get('symbol', data.get('symbol', '')),
            'side': sig.get('side', data.get('side', '')),
            'level': sig.get('level', data.get('level', '')),
            'confidence': sig.get('confidence', data.get('confidence', 0)),
            'price': data.get('price', 0),
            'cumulative_filled': data.get('cumulative_filled', 0),
            'refill_count': data.get('refill_count', 0),
            'intensity': data.get('intensity', 0),
            'key': sig.get('key', ''),
            'snippet_path': sig.get('snippet_path', ''),
            'offset': sig.get('offset', 0),
        }
        rows.append(row)

    # æŒ‰æ—¶é—´æ’åº
    rows.sort(key=lambda r: r['ts'])

    # å†™å…¥ CSV
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=CSV_FIELDS)
        writer.writeheader()
        writer.writerows(rows)

    print(f"\nâœ… å·²å¯¼å‡º {len(rows)} ä¸ªä¿¡å·åˆ°: {output_path}")

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - BUY ä¿¡å·: {sum(1 for r in rows if r['side'] == 'BUY')}")
    print(f"  - SELL ä¿¡å·: {sum(1 for r in rows if r['side'] == 'SELL')}")

    confidences = [r['confidence'] for r in rows]
    if confidences:
        print(f"  - ç½®ä¿¡åº¦èŒƒå›´: {min(confidences):.1f}% - {max(confidences):.1f}%")
        print(f"  - å¹³å‡ç½®ä¿¡åº¦: {sum(confidences) / len(confidences):.1f}%")


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("Flow Radar - ä¿¡å·æå–å·¥å…· v2.0")
    print("ä¸‰æ–¹ä¼šè°ˆç¬¬äºŒåäºŒè½®å…±è¯†")
    print("=" * 60)
    print()

    # 1. æ£€æŸ¥äº‹ä»¶ç›®å½•
    if not EVENTS_DIR.exists():
        print(f"âŒ é”™è¯¯: äº‹ä»¶ç›®å½•ä¸å­˜åœ¨: {EVENTS_DIR}")
        return

    # 2. è¯»å–æ‰€æœ‰å†°å±±ä¿¡å·
    print("Step 1: è¯»å–äº‹ä»¶æ–‡ä»¶...")
    all_signals = read_iceberg_signals(EVENTS_DIR)

    if not all_signals:
        print("âŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å†°å±±ä¿¡å·")
        return

    # 3. è¿‡æ»¤ CONFIRMED ä¿¡å·
    print("\nStep 2: è¿‡æ»¤ CONFIRMED ä¿¡å·...")
    confirmed_signals = filter_confirmed_signals(all_signals)

    if not confirmed_signals:
        print("âŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ° CONFIRMED çº§åˆ«çš„ä¿¡å·")
        print("   æç¤º: å¯èƒ½éœ€è¦ç­‰å¾… 72h éªŒè¯è¿è¡Œä¸€æ®µæ—¶é—´åå†æŠ½æ ·")
        return

    # 4. åˆ†å±‚æŠ½æ ·
    print("\nStep 3: åˆ†å±‚æŠ½æ ·...")
    sampled_signals = stratified_sampling(confirmed_signals)

    if not sampled_signals:
        print("âŒ é”™è¯¯: æŠ½æ ·å¤±è´¥")
        return

    # 5. å¯¼å‡º CSV
    print("\nStep 4: å¯¼å‡º CSV...")
    export_to_csv(sampled_signals, OUTPUT_CSV)

    print("\n" + "=" * 60)
    print("âœ… ä¿¡å·æå–å®Œæˆï¼")
    print("=" * 60)
    print(f"\nä¸‹ä¸€æ­¥ï¼š")
    print(f"1. æ‰“å¼€ {OUTPUT_CSV}")
    print(f"2. å¯¼å…¥åˆ° Google Sheet")
    print(f"3. æ·»åŠ  'annotation' åˆ—ï¼ˆHIT/MISS/UNCERTAINï¼‰")
    print(f"4. è¿›è¡Œäººå·¥æ ‡æ³¨")


if __name__ == "__main__":
    main()
