#!/usr/bin/env python3
"""
P3-2 å¤šä¿¡å·ç»¼åˆåˆ¤æ–­ç³»ç»Ÿ - å®Œæ•´æ¼”ç¤º

åŠŸèƒ½ï¼š
1. è¯»å–åŽ†å²äº‹ä»¶æ•°æ®ï¼ˆstorage/events/*.jsonl.gzï¼‰
2. æå–å†°å±±ä¿¡å·
3. ä½¿ç”¨ UnifiedSignalManager å¤„ç†
4. å±•ç¤ºä¿¡å·å…³è”ã€æŽ’åºã€åŽ»é‡å…¨æµç¨‹
5. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š

ä½œè€…ï¼šClaude Code
æ—¥æœŸï¼š2026-01-08
ç‰ˆæœ¬ï¼šv1.0
"""

import sys
from pathlib import Path
import gzip
import json
from datetime import datetime
from collections import defaultdict
from typing import List, Dict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.unified_signal_manager import UnifiedSignalManager
from core.signal_schema import IcebergSignal
from config.p3_settings import LEVEL_PRIORITY, TYPE_PRIORITY


# ==================== æ•°æ®è¯»å– ====================

def read_event_files(events_dir: Path, max_files: int = 3) -> List[Dict]:
    """
    è¯»å–äº‹ä»¶æ–‡ä»¶ä¸­çš„å†°å±±ä¿¡å·

    Args:
        events_dir: äº‹ä»¶ç›®å½•
        max_files: æœ€å¤šè¯»å–æ–‡ä»¶æ•°ï¼ˆé¿å…æ•°æ®é‡è¿‡å¤§ï¼‰

    Returns:
        å†°å±±ä¿¡å·åˆ—è¡¨
    """
    print(f"\n{'='*60}")
    print("æ­¥éª¤ 1: è¯»å–åŽ†å²äº‹ä»¶æ•°æ®")
    print(f"{'='*60}")

    event_files = sorted(events_dir.glob("*.jsonl.gz"))[-max_files:]  # åªå–æœ€æ–°çš„å‡ ä¸ªæ–‡ä»¶

    if not event_files:
        print(f"âŒ æœªæ‰¾åˆ°äº‹ä»¶æ–‡ä»¶: {events_dir}")
        return []

    print(f"æ‰¾åˆ° {len(event_files)} ä¸ªäº‹ä»¶æ–‡ä»¶ï¼ˆåªè¯»å–æœ€æ–° {max_files} ä¸ªï¼‰:")
    for f in event_files:
        print(f"  - {f.name}")

    iceberg_signals = []

    for event_file in event_files:
        try:
            with gzip.open(event_file, 'rt', encoding='utf-8') as f:
                for line_num, line in enumerate(f, start=1):
                    try:
                        event = json.loads(line.strip())

                        # åªæå–å†°å±±ä¿¡å·
                        if event.get('type') == 'iceberg':
                            iceberg_signals.append(event)

                    except json.JSONDecodeError:
                        continue

        except Exception as e:
            print(f"  è­¦å‘Š: è¯»å– {event_file.name} å¤±è´¥: {e}")
            continue

    print(f"\nâœ… æˆåŠŸè¯»å– {len(iceberg_signals)} ä¸ªå†°å±±ä¿¡å·")
    return iceberg_signals


# ==================== ä¿¡å·å¤„ç† ====================

def process_signals_demo(iceberg_dicts: List[Dict]) -> tuple:
    """
    ä½¿ç”¨ UnifiedSignalManager å¤„ç†ä¿¡å·

    Args:
        iceberg_dicts: å†°å±±ä¿¡å·å­—å…¸åˆ—è¡¨

    Returns:
        (manager, processed_signals)
    """
    print(f"\n{'='*60}")
    print("æ­¥éª¤ 2: ä½¿ç”¨ UnifiedSignalManager å¤„ç†ä¿¡å·")
    print(f"{'='*60}")

    # åˆå§‹åŒ–ç®¡ç†å™¨
    manager = UnifiedSignalManager()
    print("âœ… UnifiedSignalManager åˆå§‹åŒ–å®Œæˆ")

    # æ”¶é›†ä¿¡å·
    print(f"\næ”¶é›†ä¿¡å·...")
    signals = manager.collect_signals(icebergs=iceberg_dicts)
    print(f"âœ… æ”¶é›†åˆ° {len(signals)} ä¸ªä¿¡å·")

    # å±•ç¤ºä¿¡å·ç±»åž‹åˆ†å¸ƒ
    level_dist = defaultdict(int)
    for sig in signals:
        level_dist[sig.level] += 1

    print(f"\nä¿¡å·çº§åˆ«åˆ†å¸ƒ:")
    for level in sorted(level_dist.keys(), key=lambda l: LEVEL_PRIORITY.get(l, 999)):
        count = level_dist[level]
        pct = count / len(signals) * 100 if signals else 0
        print(f"  {level:12s}: {count:4d} ({pct:5.1f}%)")

    # å¤„ç†ä¿¡å·
    print(f"\nå¤„ç†ä¿¡å·ï¼ˆå…³è”ã€æŽ’åºã€åŽ»é‡ï¼‰...")
    processed = manager.process_signals(signals)
    print(f"âœ… å¤„ç†å®Œæˆï¼Œå¾—åˆ° {len(processed)} ä¸ªä¿¡å·")

    # ç»Ÿè®¡ä¿¡æ¯
    stats = manager.get_stats()
    print(f"\nç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»æ”¶é›†æ•°: {stats['total_collected']}")
    print(f"  åŽ»é‡æ•°é‡: {stats['deduplicated']}")
    print(f"  å…³è”æ•°é‡: {stats['correlated']}")
    print(f"  åŽ†å²å¤§å°: {stats['history_size']}")

    return manager, processed


# ==================== ç»“æžœåˆ†æž ====================

def analyze_results(processed: List[IcebergSignal]):
    """
    åˆ†æžå¤„ç†åŽçš„ä¿¡å·

    Args:
        processed: å¤„ç†åŽçš„ä¿¡å·åˆ—è¡¨
    """
    print(f"\n{'='*60}")
    print("æ­¥éª¤ 3: åˆ†æžå¤„ç†ç»“æžœ")
    print(f"{'='*60}")

    if not processed:
        print("âŒ æ²¡æœ‰ä¿¡å·å¯åˆ†æž")
        return

    # 1. ä¼˜å…ˆçº§åˆ†å¸ƒ
    print(f"\n1ï¸âƒ£ ä¼˜å…ˆçº§åˆ†å¸ƒï¼ˆæŽ’åºåŽï¼‰:")
    priority_counts = defaultdict(int)
    for sig in processed:
        priority = (sig.level, sig.signal_type)
        priority_counts[priority] += 1

    for priority, count in sorted(priority_counts.items(),
                                   key=lambda x: (LEVEL_PRIORITY.get(x[0][0], 999),
                                                 TYPE_PRIORITY.get(x[0][1], 999))):
        level, sig_type = priority
        pct = count / len(processed) * 100
        print(f"  {level:12s} / {sig_type:8s}: {count:4d} ({pct:5.1f}%)")

    # 2. ä¿¡å·å…³è”åˆ†æž
    print(f"\n2ï¸âƒ£ ä¿¡å·å…³è”åˆ†æž:")
    with_relations = [s for s in processed if s.related_signals]
    without_relations = [s for s in processed if not s.related_signals]

    print(f"  æœ‰å…³è”: {len(with_relations):4d} ({len(with_relations)/len(processed)*100:5.1f}%)")
    print(f"  æ— å…³è”: {len(without_relations):4d} ({len(without_relations)/len(processed)*100:5.1f}%)")

    if with_relations:
        relation_counts = [len(s.related_signals) for s in with_relations]
        avg_relations = sum(relation_counts) / len(relation_counts)
        max_relations = max(relation_counts)
        print(f"  å¹³å‡å…³è”æ•°: {avg_relations:.1f}")
        print(f"  æœ€å¤šå…³è”æ•°: {max_relations}")

    # 3. ç½®ä¿¡åº¦åˆ†æž
    print(f"\n3ï¸âƒ£ ç½®ä¿¡åº¦åˆ†æž:")
    confidences = [s.confidence for s in processed]
    if confidences:
        print(f"  æœ€å°å€¼: {min(confidences):.1f}%")
        print(f"  æœ€å¤§å€¼: {max(confidences):.1f}%")
        print(f"  å¹³å‡å€¼: {sum(confidences)/len(confidences):.1f}%")

        # ç½®ä¿¡åº¦åˆ†æ®µ
        high = len([c for c in confidences if c >= 85])
        mid = len([c for c in confidences if 65 <= c < 85])
        low = len([c for c in confidences if c < 65])

        print(f"  é«˜ç½®ä¿¡åº¦ (â‰¥85%): {high:4d} ({high/len(confidences)*100:5.1f}%)")
        print(f"  ä¸­ç½®ä¿¡åº¦ (65-85%): {mid:4d} ({mid/len(confidences)*100:5.1f}%)")
        print(f"  ä½Žç½®ä¿¡åº¦ (<65%): {low:4d} ({low/len(confidences)*100:5.1f}%)")

    # 4. ä¹°å–æ–¹å‘åˆ†å¸ƒ
    print(f"\n4ï¸âƒ£ ä¹°å–æ–¹å‘åˆ†å¸ƒ:")
    buy_signals = [s for s in processed if s.side == 'BUY']
    sell_signals = [s for s in processed if s.side == 'SELL']

    print(f"  BUY:  {len(buy_signals):4d} ({len(buy_signals)/len(processed)*100:5.1f}%)")
    print(f"  SELL: {len(sell_signals):4d} ({len(sell_signals)/len(processed)*100:5.1f}%)")

    # 5. æ—¶é—´è·¨åº¦
    print(f"\n5ï¸âƒ£ æ—¶é—´è·¨åº¦:")
    timestamps = [s.ts for s in processed]
    if timestamps:
        min_ts = min(timestamps)
        max_ts = max(timestamps)
        duration = (max_ts - min_ts) / 3600  # å°æ—¶

        print(f"  å¼€å§‹æ—¶é—´: {datetime.fromtimestamp(min_ts).strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"  ç»“æŸæ—¶é—´: {datetime.fromtimestamp(max_ts).strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"  æ—¶é—´è·¨åº¦: {duration:.1f} å°æ—¶")


# ==================== ç¤ºä¾‹å±•ç¤º ====================

def show_examples(processed: List[IcebergSignal], num: int = 5):
    """
    å±•ç¤ºç¤ºä¾‹ä¿¡å·

    Args:
        processed: å¤„ç†åŽçš„ä¿¡å·åˆ—è¡¨
        num: å±•ç¤ºæ•°é‡
    """
    print(f"\n{'='*60}")
    print(f"æ­¥éª¤ 4: ç¤ºä¾‹ä¿¡å·å±•ç¤ºï¼ˆå‰ {num} ä¸ªï¼‰")
    print(f"{'='*60}")

    if not processed:
        print("âŒ æ²¡æœ‰ä¿¡å·å¯å±•ç¤º")
        return

    for i, sig in enumerate(processed[:num], 1):
        print(f"\nä¿¡å· {i}:")
        print(f"  ç±»åž‹: {sig.signal_type}")
        print(f"  çº§åˆ«: {sig.level}")
        print(f"  æ–¹å‘: {sig.side}")
        print(f"  ä»·æ ¼: {sig.price}")
        print(f"  ç½®ä¿¡åº¦: {sig.confidence:.1f}%")
        print(f"  è¡¥å•æ¬¡æ•°: {sig.refill_count}")
        print(f"  å¼ºåº¦: {sig.intensity:.2f}")
        print(f"  æ—¶é—´: {sig.get_readable_time()}")
        print(f"  Key: {sig.key}")

        if sig.related_signals:
            print(f"  å…³è”ä¿¡å· ({len(sig.related_signals)} ä¸ª):")
            for rel in sig.related_signals[:3]:  # åªæ˜¾ç¤ºå‰ 3 ä¸ª
                print(f"    â†’ {rel}")


# ==================== ç”ŸæˆæŠ¥å‘Š ====================

def generate_report(manager: UnifiedSignalManager, processed: List[IcebergSignal],
                   output_file: Path):
    """
    ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š

    Args:
        manager: UnifiedSignalManager å®žä¾‹
        processed: å¤„ç†åŽçš„ä¿¡å·åˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    print(f"\n{'='*60}")
    print("æ­¥éª¤ 5: ç”ŸæˆæŠ¥å‘Š")
    print(f"{'='*60}")

    output_file.parent.mkdir(parents=True, exist_ok=True)

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# P3-2 å¤šä¿¡å·ç»¼åˆåˆ¤æ–­ç³»ç»Ÿ - æ¼”ç¤ºæŠ¥å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        # ç»Ÿè®¡ä¿¡æ¯
        stats = manager.get_stats()
        f.write("## ç»Ÿè®¡ä¿¡æ¯\n\n")
        f.write(f"- æ€»æ”¶é›†æ•°: {stats['total_collected']}\n")
        f.write(f"- å¤„ç†åŽæ•°é‡: {len(processed)}\n")
        f.write(f"- åŽ»é‡æ•°é‡: {stats['deduplicated']}\n")
        f.write(f"- å…³è”æ•°é‡: {stats['correlated']}\n")
        f.write(f"- åŽ»é‡çŽ‡: {stats['deduplicated']/stats['total_collected']*100:.1f}%\n\n")

        # ä¼˜å…ˆçº§åˆ†å¸ƒ
        f.write("## ä¼˜å…ˆçº§åˆ†å¸ƒ\n\n")
        f.write("| çº§åˆ« | ç±»åž‹ | æ•°é‡ | å æ¯” |\n")
        f.write("|------|------|------|------|\n")

        priority_counts = defaultdict(int)
        for sig in processed:
            priority = (sig.level, sig.signal_type)
            priority_counts[priority] += 1

        for priority, count in sorted(priority_counts.items(),
                                       key=lambda x: (LEVEL_PRIORITY.get(x[0][0], 999),
                                                     TYPE_PRIORITY.get(x[0][1], 999))):
            level, sig_type = priority
            pct = count / len(processed) * 100 if processed else 0
            f.write(f"| {level} | {sig_type} | {count} | {pct:.1f}% |\n")

        # ä¿¡å·ç¤ºä¾‹
        f.write("\n## ä¿¡å·ç¤ºä¾‹ï¼ˆå‰ 10 ä¸ªï¼‰\n\n")
        for i, sig in enumerate(processed[:10], 1):
            f.write(f"### ä¿¡å· {i}\n\n")
            f.write(f"- **çº§åˆ«**: {sig.level}\n")
            f.write(f"- **æ–¹å‘**: {sig.side}\n")
            f.write(f"- **ä»·æ ¼**: {sig.price}\n")
            f.write(f"- **ç½®ä¿¡åº¦**: {sig.confidence:.1f}%\n")
            f.write(f"- **è¡¥å•æ¬¡æ•°**: {sig.refill_count}\n")
            f.write(f"- **å¼ºåº¦**: {sig.intensity:.2f}\n")
            f.write(f"- **æ—¶é—´**: {sig.get_readable_time()}\n")

            if sig.related_signals:
                f.write(f"- **å…³è”ä¿¡å·**: {len(sig.related_signals)} ä¸ª\n")

            f.write("\n")

    print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("P3-2 å¤šä¿¡å·ç»¼åˆåˆ¤æ–­ç³»ç»Ÿ - å®Œæ•´æ¼”ç¤º")
    print("="*60)
    print("\nè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¼”ç¤ºï¼Œå±•ç¤º P3-2 æž¶æž„çš„å®Œæ•´æµç¨‹ï¼š")
    print("1. è¯»å–åŽ†å²äº‹ä»¶æ•°æ®")
    print("2. ä½¿ç”¨ UnifiedSignalManager å¤„ç†")
    print("3. åˆ†æžå¤„ç†ç»“æžœ")
    print("4. å±•ç¤ºç¤ºä¾‹ä¿¡å·")
    print("5. ç”ŸæˆæŠ¥å‘Š")

    # é…ç½®
    events_dir = Path("storage/events")
    output_file = Path("docs/p3_demo_report.md")

    # æ£€æŸ¥ç›®å½•
    if not events_dir.exists():
        print(f"\nâŒ é”™è¯¯: äº‹ä»¶ç›®å½•ä¸å­˜åœ¨: {events_dir}")
        print("   è¯·ç¡®ä¿ 72h éªŒè¯å·²è¿è¡Œå¹¶ç”Ÿæˆäº†äº‹ä»¶æ–‡ä»¶")
        return 1

    # æ­¥éª¤ 1: è¯»å–æ•°æ®
    iceberg_dicts = read_event_files(events_dir, max_files=2)  # åªè¯» 2 ä¸ªæ–‡ä»¶é¿å…æ•°æ®é‡è¿‡å¤§

    if not iceberg_dicts:
        print("\nâŒ æ²¡æœ‰å¯ç”¨çš„å†°å±±ä¿¡å·æ•°æ®")
        return 1

    # æ­¥éª¤ 2: å¤„ç†ä¿¡å·
    manager, processed = process_signals_demo(iceberg_dicts)

    if not processed:
        print("\nâŒ ä¿¡å·å¤„ç†å¤±è´¥")
        return 1

    # æ­¥éª¤ 3: åˆ†æžç»“æžœ
    analyze_results(processed)

    # æ­¥éª¤ 4: å±•ç¤ºç¤ºä¾‹
    show_examples(processed, num=5)

    # æ­¥éª¤ 5: ç”ŸæˆæŠ¥å‘Š
    generate_report(manager, processed, output_file)

    # å®Œæˆ
    print(f"\n{'='*60}")
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print(f"{'='*60}")
    print(f"\nðŸ“Š æŠ¥å‘Šä½ç½®: {output_file}")
    print(f"\nðŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"   1. æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Š")
    print(f"   2. æ ¹æ®æŠ¥å‘Šè°ƒæ•´é…ç½®å‚æ•°")
    print(f"   3. è€ƒè™‘å®žé™…é›†æˆåˆ° alert_monitor.py")

    return 0


if __name__ == "__main__":
    exit(main())
